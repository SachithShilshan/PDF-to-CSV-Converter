# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nvopWeOBaGo_oj2KWqy_FCqgS7DDZO6U
"""

import tkinter as tk
from tkinter import filedialog, messagebox
import pdfplumber
import pandas as pd
import os

def extract_data_from_pdf(pdf_path):
    # Initialize an empty list to store tables
    all_tables = []

    # Extract tables from the PDF
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            tables = page.extract_table()
            if tables:
                df = pd.DataFrame(tables)  # Convert to DataFrame
                all_tables.append(df)

    # Combine extracted tables if available
    if all_tables:
        df_combined = pd.concat(all_tables, ignore_index=True)

        # Ensure 'Category' column exists before assignment
        if "Category" not in df_combined.columns:
            df_combined["Category"] = ""

        # If the first column has 'Level', set 'Category' column to 'LevelX'
        df_combined.loc[df_combined.iloc[:, 0] == "Level", "Category"] = "LevelX"

        # Ensure correct indexing for the first column
        first_col_name = df_combined.columns[0]

        # If the first column has 'Level', set 'Category' to 'LevelX'
        df_combined.loc[df_combined[first_col_name] == "Level", "Category"] = "LevelX"

        # Fill down (forward fill) the 'Category' column
        df_combined["Category"] = df_combined["Category"].replace("", pd.NA).ffill()

        # Filter rows where 'Category' is 'LevelX'
        df_combined = df_combined[df_combined["Category"] == "LevelX"]

    # Define function to apply conditional logic safely
    def add_custom_column(row):
        # Ensure the row has enough columns before checking conditions
        col_count = len(row)

        # Define safe index access with default value
        get_value = lambda idx: row[idx] if idx < col_count else None

        # Check conditions dynamically
        if any(get_value(i) == "Color" for i in range(4, 9)):  # Columns 5-9 (0-indexed as 4-8)
            return "ColorX"
        elif get_value(5) == "Description":  # Column 6 (0-indexed as 5)
            return "DescriptionX"
        else:
            return None

    # Apply function safely
    df_combined["Custom"] = df_combined.apply(add_custom_column, axis=1)

    def add_custom_column2(row):
        # Ensure the row has enough columns before checking conditions
        col_count = len(row)

        # Define safe index access with default value
        get_value = lambda idx: row[idx] if idx < col_count else None

        # Check conditions dynamically
        for i in range(4, 9):  # Columns 5-9 (0-indexed as 4-8)
            if get_value(i) == "Color":
                return i  # Return column index (1-based)

        if get_value(5) == "Description":  # Column 6 (0-indexed as 5)
            return 0

        return None  # Ensure a valid return

    # Apply function safely
    df_combined["ColoNo"] = df_combined.apply(add_custom_column2, axis=1)

    # Forward fill missing values
    df_combined["Custom"] = df_combined["Custom"].fillna(method="ffill")
    df_combined["ColoNo"] = df_combined["ColoNo"].fillna(method="ffill")

    # Fill first six columns with forward fill
    df_combined.iloc[:, :6] = df_combined.iloc[:, :6].fillna(method="ffill")

    # Filter only rows where 'Custom' is 'Description'
    df_combined_Dsc = df_combined[df_combined["Custom"] == "DescriptionX"].copy()

    # Ensure the DataFrame is not empty before promoting the first row as header
    if not df_combined_Dsc.empty:
        df_combined_Dsc.columns = df_combined_Dsc.iloc[0]  # Set first row as column names
        df_combined_Dsc = df_combined_Dsc[1:].reset_index(drop=True)  # Remove the first row

        # Ensure 'Level' column exists before filtering
        if "Level" in df_combined_Dsc.columns:
            df_combined_Dsc = df_combined_Dsc[df_combined_Dsc["Level"] == "1"]
        else:
            print("Warning: 'Level' column not found in DataFrame!")
    else:
        print("Warning: No rows found with 'Custom' == 'Description'")

    # List of columns to drop
    columns_to_remove = [col for col in df_combined_Dsc.columns if pd.isna(col) or col in ["LevelX", "DescriptionX", 0]]

    # Drop the columns
    df_combined_Dsc = df_combined_Dsc.drop(columns=columns_to_remove, errors="ignore")

    # Color processing
    df_combined_Color = df_combined[df_combined["Custom"] == "ColorX"].copy()

    # Initialize an empty list to collect transformed DataFrames
    df_list = []

    # Loop over the range of j from 4 to 9
    for j in range(4, 10):
        # Step 1: Filter rows where 'ColoNo' is j
        df_combined_Color_j = df_combined_Color[df_combined_Color["ColoNo"] == j].copy()

        # Check if the DataFrame is empty after filtering
        if df_combined_Color_j.empty:
            print(f"No data available for ColoNo = {j}. Skipping this iteration.")
            continue  # Skip the current iteration if the DataFrame is empty

        # Step 2: Transpose the table (first row becomes column names)
        df_transposed = df_combined_Color_j.T.reset_index(drop=True)  # Transpose and reset index

        # Step 4: Ensure the first column is explicitly named "Column1"
        df_transposed = df_transposed.rename(columns={df_transposed.columns[0]: "Column1"})

        # Step 5: Remove rows where 'Column1' is null, 'ColorX', or 'LevelX', or equal to the value of j
        df_transposed = df_transposed[
            (df_transposed["Column1"].notna()) &
            (df_transposed["Column1"] != "ColorX") &
            (df_transposed["Column1"] != "LevelX") &
            (df_transposed["Column1"] != j)  # Convert j to string for comparison
        ].copy()

        # Check if the DataFrame is empty after filtering
        if df_transposed.empty:
            print(f"Data after filtering is empty for ColoNo = {j}. Skipping this iteration.")
            continue  # Skip this iteration if the DataFrame is empty after filtering

        # Step 6: Trim text in 'Column1'
        df_transposed["Column1"] = df_transposed["Column1"].astype(str).str.strip()

        # Step 7: Clean text (remove extra spaces)
        df_transposed["Column1"] = df_transposed["Column1"].str.replace(r"\s+", " ", regex=True)

        # Step 8: Replace missing values in 'Column1' with "No"
        df_transposed["Column1"] = df_transposed["Column1"].fillna("No")

        # **Step 9: Re-Transpose the table again**
        df_transposed = df_transposed.T  # Transpose again to get the original table structure

        # **Step 10: Reset column names after re-transposing**
        df_transposed.columns = [f"Column{i}" for i in range(df_transposed.shape[1])]

        # Step 11: Promote the first row as column headers again
        new_columns = df_transposed.iloc[0].astype(str)  # Convert to string to avoid issues
        new_columns = new_columns.str.strip()  # Remove leading/trailing spaces

        # Detect duplicate column names and rename them uniquely
        column_counts = {}
        unique_columns = []
        for col in new_columns:
            if col in column_counts:
                column_counts[col] += 1
                unique_columns.append(f"{col}_{column_counts[col]}")  # Append count for duplicates
            else:
                column_counts[col] = 0
                unique_columns.append(col)

        df_transposed.columns = unique_columns  # Assign the updated column names
        df_transposed = df_transposed[1:].reset_index(drop=True)  # Remove first row

        # **Step 12: Filter by first column where 'Level' is equal to "1"**
        if "Level" in df_transposed.columns:
            df_transposed = df_transposed[df_transposed["Level"] == "1"].copy()

        # âœ… Append the transformed DataFrame to the list
        df_list.append(df_transposed)

    # Step 13: Concatenate all DataFrames in df_list into a single DataFrame (aligning columns)
    if df_list:
        final_df = pd.concat(df_list, ignore_index=True, sort=False)  # sort=False ensures columns stay in original order
        print("All tables have been processed and concatenated.")
    else:
        print("No valid data to concatenate.")

    # Merge the final DataFrame with df_combined_Dsc
    final_df.columns = final_df.columns.str.strip().str.replace(r"\s+", " ", regex=True).str.replace(" ", "").str.lower()
    df_combined_Dsc.columns = df_combined_Dsc.columns.str.strip().str.replace(r"\s+", " ", regex=True).str.replace(" ", "").str.lower()

    # Drop NaN columns from df_combined_Dsc (if any)
    df_combined_Dsc = df_combined_Dsc.dropna(axis=1, how="all")

    # Merge using standardized column names
    merged_df = pd.merge(
        df_combined_Dsc, final_df,
        on=["level", "itempart", "itempartcomments", "mainmaterial", "product"],
        how="left"
    )
    return merged_df



def upload_pdf():
    file_path = filedialog.askopenfilename(filetypes=[("PDF Files", "*.pdf")])
    if file_path:
        try:
            df = extract_data_from_pdf(file_path)
            if not df.empty:
                pdf_name = os.path.splitext(os.path.basename(file_path))[0]  # Extract filename without extension
                save_path = filedialog.asksaveasfilename(initialfile=f"{pdf_name}.csv", defaultextension=".csv", filetypes=[("CSV Files", "*.csv")])
                if save_path:
                    df.to_csv(save_path, index=False)
                    messagebox.showinfo("Success", f"Data saved successfully: {save_path}")
            else:
                messagebox.showerror("Error", "No tabular data found in PDF")
        except Exception as e:
            messagebox.showerror("Error", f"An error occurred: {e}")

# GUI Setup
root = tk.Tk()
root.title("PDF to CSV Converter")
root.geometry("400x200")

btn_upload = tk.Button(root, text="Upload PDF", command=upload_pdf, padx=10, pady=5)
btn_upload.pack(pady=20)

root.mainloop()